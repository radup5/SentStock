{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8ob6w2AyrIVf"
      },
      "source": [
        "TO DO\n",
        "\n",
        "* refactor validation and test steps\n",
        "* probably need the finbert model in training mode too when training?\n",
        "* keeping LSTM for classification?\n",
        "* needs some hyperparameter tuning\n",
        "* modify any of the args: get_text_split(text, length=200, overlap=50, max_chunks=4)?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7cxggRBXn4Ne"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "H8Ri7UEW2wMk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "g8kFjn0iGuCq"
      },
      "outputs": [],
      "source": [
        "# %pip install datasets\n",
        "# %pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "urPT0BCWHr3q"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "r1WrqP_qHU9R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "sEXqEJ6X3wzQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected device is cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Selected device is {}'.format(device))\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "finbert = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
        "# bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# distilbert = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lB2Jbs42Uc4J"
      },
      "source": [
        "**SET THE MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "7qTrC0eLUZ-o"
      },
      "outputs": [],
      "source": [
        "model = finbert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint_dir = os.path.join(os.getcwd(),'checkpoints')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l0D4PPi_Gd_b"
      },
      "source": [
        "# Data loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "9RxZlK3sdRPj",
        "outputId": "78876214-45f9-4b05-fa4c-f2fe1b5f9328"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>returns</th>\n",
              "      <th>trend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>Agilent (A) Q2 Earnings Match Estimates, Reven...</td>\n",
              "      <td>Agilent Technologies A reported second-quarter...</td>\n",
              "      <td>-5.620333</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-06-02</td>\n",
              "      <td>Agilent Intelligent Mass Spectrometry Solution...</td>\n",
              "      <td>Self-aware solutions making mass spectrometry ...</td>\n",
              "      <td>1.995527</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-11-20</td>\n",
              "      <td>Agilent Reports Fourth-Quarter Fiscal Year 202...</td>\n",
              "      <td>Revenue at the high end of guidance and EPS ex...</td>\n",
              "      <td>7.344231</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAL</td>\n",
              "      <td>2023-05-22</td>\n",
              "      <td>The Undoing of American and JetBlue’s Northeas...</td>\n",
              "      <td>American Airlines CEO Robert Isom (left) speak...</td>\n",
              "      <td>-3.171385</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAL</td>\n",
              "      <td>2023-05-25</td>\n",
              "      <td>American Airlines flags no earnings impact fro...</td>\n",
              "      <td>(Reuters) - American Airlines Group Inc's Chie...</td>\n",
              "      <td>4.420287</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3283</th>\n",
              "      <td>ZION</td>\n",
              "      <td>2023-08-07</td>\n",
              "      <td>Why Zions Bancorp Stock Surged 42% in July</td>\n",
              "      <td>Zions Bancorporation (NASDAQ: ZION) rewarded i...</td>\n",
              "      <td>-3.270037</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3284</th>\n",
              "      <td>ZION</td>\n",
              "      <td>2023-11-14</td>\n",
              "      <td>Regional Bank Stocks Rise After Bond Yields Drop</td>\n",
              "      <td>One big winner from Tuesday’s surprise inflati...</td>\n",
              "      <td>8.101503</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3285</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-06-15</td>\n",
              "      <td>Zoetis Releases 2022 Sustainability Report to ...</td>\n",
              "      <td>$7.4 million in corporate giving invested in c...</td>\n",
              "      <td>3.972232</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3286</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-08-08</td>\n",
              "      <td>Zoetis (ZTS) Q2 2023 Earnings Call Transcript ...</td>\n",
              "      <td>Hosting the call today is Steve Frank, vice pr...</td>\n",
              "      <td>5.119265</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3287</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-08-10</td>\n",
              "      <td>USNA or ZTS: Which Is the Better Value Stock R...</td>\n",
              "      <td>Investors interested in stocks from the Medica...</td>\n",
              "      <td>-0.459472</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3288 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     symbol publish_time                                              title  \\\n",
              "0         A   2023-05-24  Agilent (A) Q2 Earnings Match Estimates, Reven...   \n",
              "1         A   2023-06-02  Agilent Intelligent Mass Spectrometry Solution...   \n",
              "2         A   2023-11-20  Agilent Reports Fourth-Quarter Fiscal Year 202...   \n",
              "3       AAL   2023-05-22  The Undoing of American and JetBlue’s Northeas...   \n",
              "4       AAL   2023-05-25  American Airlines flags no earnings impact fro...   \n",
              "...     ...          ...                                                ...   \n",
              "3283   ZION   2023-08-07         Why Zions Bancorp Stock Surged 42% in July   \n",
              "3284   ZION   2023-11-14   Regional Bank Stocks Rise After Bond Yields Drop   \n",
              "3285    ZTS   2023-06-15  Zoetis Releases 2022 Sustainability Report to ...   \n",
              "3286    ZTS   2023-08-08  Zoetis (ZTS) Q2 2023 Earnings Call Transcript ...   \n",
              "3287    ZTS   2023-08-10  USNA or ZTS: Which Is the Better Value Stock R...   \n",
              "\n",
              "                                                   body   returns     trend  \n",
              "0     Agilent Technologies A reported second-quarter... -5.620333  decrease  \n",
              "1     Self-aware solutions making mass spectrometry ...  1.995527    stable  \n",
              "2     Revenue at the high end of guidance and EPS ex...  7.344231  increase  \n",
              "3     American Airlines CEO Robert Isom (left) speak... -3.171385  decrease  \n",
              "4     (Reuters) - American Airlines Group Inc's Chie...  4.420287  increase  \n",
              "...                                                 ...       ...       ...  \n",
              "3283  Zions Bancorporation (NASDAQ: ZION) rewarded i... -3.270037  decrease  \n",
              "3284  One big winner from Tuesday’s surprise inflati...  8.101503  increase  \n",
              "3285  $7.4 million in corporate giving invested in c...  3.972232  increase  \n",
              "3286  Hosting the call today is Steve Frank, vice pr...  5.119265  increase  \n",
              "3287  Investors interested in stocks from the Medica... -0.459472    stable  \n",
              "\n",
              "[3288 rows x 6 columns]"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv('dataset.csv')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "SBg2eobOeQ-H"
      },
      "outputs": [],
      "source": [
        "df = dataset[['symbol', 'title', 'body', 'trend']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TegDd_AP6Q5c"
      },
      "outputs": [],
      "source": [
        "def get_smaller_dataset(df, size=0.25):\n",
        "\n",
        "  df = df.dropna()\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=size, random_state=42)\n",
        "  X = df.drop('trend', axis=1)\n",
        "  y = df['trend']\n",
        "\n",
        "  for train_index, test_index in sss.split(X, y):\n",
        "      X_sample, y_sample = X.iloc[test_index], y.iloc[test_index]\n",
        "\n",
        "  sample_df = X_sample.assign(trend=y_sample.values)\n",
        "  sample_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "  return sample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "93jQ0RKJ6kWF",
        "outputId": "5fe7e10f-9214-408d-d387-8dacadfd7843"
      },
      "outputs": [],
      "source": [
        "df = get_smaller_dataset(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tNAimq0i3Sq5"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "vrfxnago3UNv"
      },
      "outputs": [],
      "source": [
        "def wrap_tokenizer(tokenizer, padding=True, truncation=True, return_tensors='pt', max_length=None):\n",
        "    def tokenize(text):\n",
        "        text = list(text)\n",
        "        tokens = tokenizer(\n",
        "            text,\n",
        "            padding=padding,\n",
        "            return_attention_mask=False,\n",
        "            truncation=truncation,\n",
        "            max_length=max_length,\n",
        "            return_tensors=return_tensors\n",
        "            )['input_ids']\n",
        "        return tokens\n",
        "    return tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DUM2sou53cE7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (752 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "tokenize = wrap_tokenizer(tokenizer, padding=False, truncation = False, return_tensors=None)\n",
        "tokens = tokenize(df['body'])\n",
        "num_tokens = [len(x) for x in tokens]\n",
        "df['length'] = pd.Series(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GObBDlbl3gD8",
        "outputId": "fbcce4b7-744b-45fa-a9be-31f650949d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Max length of news body:  16358\n",
            "Number of text that have more than 1*max_tokens is 662\n",
            "Number of text that have more than 2*max_tokens is 429\n",
            "Number of text that have more than 3*max_tokens is 275\n",
            "Number of text that have more than 4*max_tokens is 197\n",
            "Number of text that have more than 5*max_tokens is 148\n",
            "Number of text that have more than 6*max_tokens is 100\n",
            "Number of text that have more than 7*max_tokens is 73\n",
            "Number of text that have more than 8*max_tokens is 53\n",
            "Number of text that have more than 9*max_tokens is 38\n",
            "Number of text that have more than 10*max_tokens is 34\n",
            "Number of text that have more than 11*max_tokens is 24\n",
            "Number of text that have more than 12*max_tokens is 19\n",
            "Number of text that have more than 13*max_tokens is 16\n",
            "Number of text that have more than 14*max_tokens is 10\n",
            "Number of text that have more than 15*max_tokens is 10\n",
            "Number of text that have more than 16*max_tokens is 9\n",
            "Number of text that have more than 17*max_tokens is 7\n",
            "Number of text that have more than 18*max_tokens is 5\n",
            "Number of text that have more than 19*max_tokens is 4\n",
            "Number of text that have more than 20*max_tokens is 4\n",
            "Number of text that have more than 21*max_tokens is 2\n",
            "Number of text that have more than 22*max_tokens is 2\n",
            "Number of text that have more than 23*max_tokens is 2\n",
            "Number of text that have more than 24*max_tokens is 1\n",
            "Number of text that have more than 25*max_tokens is 1\n",
            "Number of text that have more than 26*max_tokens is 1\n",
            "Number of text that have more than 27*max_tokens is 1\n",
            "Number of text that have more than 28*max_tokens is 1\n",
            "Number of text that have more than 29*max_tokens is 1\n",
            "Number of text that have more than 30*max_tokens is 1\n"
          ]
        }
      ],
      "source": [
        "max_length = df['length'].unique().max()\n",
        "print(\"Max length of news body: \", max_length)\n",
        "max_tokens = 512\n",
        "for i in range(1,(max_length//max_tokens)):\n",
        "    num = sum(df['length']>i*max_tokens)\n",
        "    print(f\"Number of text that have more than {i}*max_tokens is {num}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "WIBus79rsMA8"
      },
      "outputs": [],
      "source": [
        "def get_text_split(text, length=200, overlap=50, max_chunks=4):\n",
        "    chunks = []\n",
        "\n",
        "    words = text.split()\n",
        "    n_words = len(words)\n",
        "\n",
        "    n = max(1, min(max_chunks, (n_words - length) // (length - overlap) + 1))\n",
        "\n",
        "    for i in range(n):\n",
        "        start_idx = i * (length - overlap)\n",
        "        end_idx = min(start_idx + length, n_words)\n",
        "\n",
        "        chunk_words = words[start_idx:end_idx]\n",
        "\n",
        "        chunk_text = \" \".join(chunk_words)\n",
        "\n",
        "        # If it's the last chunk and its length is less than 75% of the desired length, skip\n",
        "        if i == n - 1 and len(chunk_words) < 0.75 * length and n > 1:\n",
        "            continue\n",
        "\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lxsiO2hW3r0w"
      },
      "source": [
        "**ENCODE LABELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "wEGeEU453rR9"
      },
      "outputs": [],
      "source": [
        "labels = [\"increase\", \"stable\", \"decrease\"]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['trend'] = label_encoder.fit_transform(df['trend'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8fUZch4EU5_X"
      },
      "source": [
        "# Train-val-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ln0P6JyJVDRV"
      },
      "outputs": [],
      "source": [
        "def split_df(df):\n",
        "    n_rows = len(df)\n",
        "\n",
        "    df_train = df.iloc[:int(0.8*n_rows),:]\n",
        "    df_val = df.iloc[int(0.8*n_rows):int(0.9*n_rows),:]\n",
        "    df_test = df.iloc[int(0.9*n_rows):,:]\n",
        "\n",
        "    return df_train, df_val, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17TPVuLvVBZz",
        "outputId": "b4f2c83c-592e-440c-f492-8aa35e3f627f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 652\n",
            "Number of validation examples: 82\n",
            "Number of test examples: 83\n"
          ]
        }
      ],
      "source": [
        "n_rows = len(df)\n",
        "dfs_train, dfs_val, dfs_test = [],[],[]\n",
        "gb = df.groupby('trend')\n",
        "for x in gb.groups:\n",
        "    group = gb.get_group(x)\n",
        "    df_train, df_val, df_test = split_df(group)\n",
        "    dfs_train.append(df_train)\n",
        "    dfs_val.append(df_val)\n",
        "    dfs_test.append(df_test)\n",
        "\n",
        "df_train = pd.concat(dfs_train, ignore_index=True)\n",
        "\n",
        "df_val = pd.concat(dfs_val, ignore_index=True)\n",
        "\n",
        "df_test = pd.concat(dfs_test, ignore_index=True)\n",
        "\n",
        "print(f'Number of training examples: {len(df_train)}')\n",
        "print(f'Number of validation examples: {len(df_val)}')\n",
        "print(f'Number of test examples: {len(df_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "WanNgS0RU5Ns"
      },
      "outputs": [],
      "source": [
        "df_train.body = df_train.body.apply(lambda x: get_text_split(x))\n",
        "df_val.body = df_val.body.apply(lambda x: get_text_split(x))\n",
        "df_test.body = df_test.body.apply(lambda x: get_text_split(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ZJwTu6xVVHrn"
      },
      "outputs": [],
      "source": [
        "df_train['n_chunks'] = df_train.body.apply(lambda x: len(x))\n",
        "df_val['n_chunks'] = df_val.body.apply(lambda x: len(x))\n",
        "df_test['n_chunks'] = df_test.body.apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "8_1IexNxymQO",
        "outputId": "a3b1fd9e-520d-47fe-8117-71901a243b39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>trend</th>\n",
              "      <th>length</th>\n",
              "      <th>n_chunks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TTI</td>\n",
              "      <td>TETRA Technologies Insider Buyers Net US$98k D...</td>\n",
              "      <td>[Insiders who bought TETRA Technologies, Inc. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>799</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RSG</td>\n",
              "      <td>Republic Services, Inc. (NYSE:RSG) Delivered A...</td>\n",
              "      <td>[Many investors are still learning about the v...</td>\n",
              "      <td>0</td>\n",
              "      <td>1679</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GSIT</td>\n",
              "      <td>GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ear...</td>\n",
              "      <td>[GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ea...</td>\n",
              "      <td>0</td>\n",
              "      <td>2126</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ARE</td>\n",
              "      <td>Alexandria Real Estate Equities, Inc.'s Discip...</td>\n",
              "      <td>[PASADENA, Calif., May 24, 2023 /PRNewswire/ -...</td>\n",
              "      <td>0</td>\n",
              "      <td>1437</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MO</td>\n",
              "      <td>Altria (MO) Gains on Pricing Power Amid Low Ci...</td>\n",
              "      <td>[Altria Group, Inc. MO looks well-positioned d...</td>\n",
              "      <td>0</td>\n",
              "      <td>1328</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  symbol                                              title  \\\n",
              "0    TTI  TETRA Technologies Insider Buyers Net US$98k D...   \n",
              "1    RSG  Republic Services, Inc. (NYSE:RSG) Delivered A...   \n",
              "2   GSIT  GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ear...   \n",
              "3    ARE  Alexandria Real Estate Equities, Inc.'s Discip...   \n",
              "4     MO  Altria (MO) Gains on Pricing Power Amid Low Ci...   \n",
              "\n",
              "                                                body  trend  length  n_chunks  \n",
              "0  [Insiders who bought TETRA Technologies, Inc. ...      0     799         3  \n",
              "1  [Many investors are still learning about the v...      0    1679         4  \n",
              "2  [GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ea...      0    2126         4  \n",
              "3  [PASADENA, Calif., May 24, 2023 /PRNewswire/ -...      0    1437         4  \n",
              "4  [Altria Group, Inc. MO looks well-positioned d...      0    1328         4  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KKswckthV_qX"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "# def set_lr(optimizer, lr):\n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         param_group['lr'] = lr\n",
        "\n",
        "\n",
        "def save_to_disk(txt_path, values):\n",
        "    if os.path.isfile(txt_path):\n",
        "        os.remove(txt_path)\n",
        "    with open(txt_path, \"wb\") as fp:\n",
        "        pickle.dump(values, fp)\n",
        "    return\n",
        "\n",
        "\n",
        "# def load_from_disk(txt_path):\n",
        "#     with open(txt_path, \"rb\") as f:\n",
        "#         values =  pickle.load(f)\n",
        "#     return values\n",
        "\n",
        "\n",
        "def save_checkpoint(model, classifier, optimizer, logs, epoch):\n",
        "    print('')\n",
        "    print('Saving checkpoint...')\n",
        "    state_dict = {\n",
        "        'model': model.state_dict(),\n",
        "        'classifier':classifier.state_dict(),\n",
        "        'optimizer': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(state_dict, os.path.join(checkpoint_dir, 'checkpoint_{}.pt'.format(epoch)))\n",
        "    save_to_disk(os.path.join(checkpoint_dir, 'logs.txt'),logs)\n",
        "    print(f'Checkpoint saved!')\n",
        "\n",
        "\n",
        "# def load_checkpoint(checkpoint_dir, epoch, xlmr, classifier, device, optimizer=None):\n",
        "#     pretrained_dict = torch.load(os.path.join(checkpoint_dir,'checkpoint_{}.pt'.format(epoch)),map_location=torch.device(device))\n",
        "#     classifier.load_state_dict(pretrained_dict['classifier'])\n",
        "#     if optimizer is not None:\n",
        "#         optimizer.load_state_dict(pretrained_dict['optimizer'])\n",
        "#         return classifier, optimizer\n",
        "#     return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "VzmXJzFxWAez"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.n_chunks = df['n_chunks'].to_list()\n",
        "        self.X = df['body'].to_list()\n",
        "        self.Y = df['trend']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.X[index], self.Y.iloc[index], self.n_chunks[index]\n",
        "\n",
        "def collate_func(batch):\n",
        "    X = [x[0] for x in batch]\n",
        "    Y = torch.Tensor([x[1] for x in batch])\n",
        "    c = [x[2] for x in batch]\n",
        "    return [X,Y,c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "IU-F8-RijOJL"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, lstm_size, emb_dim, out_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=lstm_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(in_features=lstm_size, out_features=out_dim)\n",
        "\n",
        "    def forward(self, x, n_chunks):\n",
        "        x = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "        x = pack_padded_sequence(input=x, lengths=n_chunks, batch_first=True, enforce_sorted=False)\n",
        "        x, _ = self.lstm(x)\n",
        "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
        "        x = x[:,-1,:]\n",
        "        x = self.dropout(x)\n",
        "        logit = self.linear(x)\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "WAY544k5eIDU"
      },
      "outputs": [],
      "source": [
        "# class Classifier(nn.Module):\n",
        "#     def __init__(self, bert_model, out_dim_lin, dropout):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.bert = bert_model\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.linear = nn.Linear(in_features=self.bert.config.hidden_size, out_features=out_dim_lin)\n",
        "\n",
        "#     def forward(self, x, n_chunks):\n",
        "\n",
        "#         x = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             bert_outputs = self.bert(x)\n",
        "\n",
        "#         bert_last_hidden_state = bert_outputs.last_hidden_state\n",
        "\n",
        "#         x = self.dropout(bert_last_hidden_state)\n",
        "\n",
        "#         logit = self.linear(x[:, 0, :])\n",
        "\n",
        "#         return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "yrvvsE6hW0c1"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, tokenize, model, classifier, optimizer, scheduler, logs, dropout=0.0):\n",
        "    classifier.train()\n",
        "    model.train()\n",
        "    #model.eval()\n",
        "\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "\n",
        "    for text, target, n_chunks in train_loader:\n",
        "        logs['lr'].append(get_lr(optimizer))\n",
        "\n",
        "        target = target.long().to(device)\n",
        "\n",
        "        flat_text = [item for sublist in text for item in sublist]\n",
        "        tokens = tokenize(flat_text)\n",
        "        tokens = tokens.to(device)\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #   outputs = model(tokens)\n",
        "\n",
        "        outputs = model(tokens)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "\n",
        "        pooled_emb = torch.mean(embeddings, axis=1)\n",
        "        pooled_emb = nn.Dropout(dropout)(pooled_emb)\n",
        "        x = [s for s in torch.split(pooled_emb, n_chunks, dim=0)]\n",
        "\n",
        "        logits = classifier(x, n_chunks)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(input=logits, target=target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, predictions = torch.max(logits, 1)\n",
        "        correct_predictions = torch.sum(predictions == target).item()\n",
        "        acc = correct_predictions / target.size(0)\n",
        "        accuracy.append(acc)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return accuracy, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "cTqXzoqdWEn-"
      },
      "outputs": [],
      "source": [
        "def val_step(val_loader, tokenize, model, classifier, device, is_test=False):\n",
        "\n",
        "    classifier.eval()\n",
        "    model.eval()\n",
        "\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, target, n_chunks in val_loader:\n",
        "            target = target.long().to(device)\n",
        "\n",
        "            flat_text = [item for sublist in text for item in sublist]\n",
        "            tokens = tokenize(flat_text)\n",
        "            tokens = tokens.to(device)\n",
        "\n",
        "            outputs = model(tokens)\n",
        "            embeddings = outputs.last_hidden_state\n",
        "\n",
        "            pooled_emb = torch.mean(embeddings, axis=1)\n",
        "            x = [s for s in torch.split(pooled_emb, n_chunks, dim=0)]\n",
        "\n",
        "            logits = classifier(x, n_chunks)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(input=logits, target=target)\n",
        "\n",
        "            _, predictions = torch.max(logits, 1)\n",
        "            correct_predictions = torch.sum(predictions == target).item()\n",
        "            acc = correct_predictions / target.size(0)\n",
        "            accuracy.append(acc)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            if is_test:\n",
        "                all_predictions.append(predictions.cpu())\n",
        "                all_targets.append(target.cpu())\n",
        "\n",
        "    if is_test:\n",
        "        all_predictions = torch.cat(all_predictions).numpy()\n",
        "        all_targets = torch.cat(all_targets).numpy()\n",
        "        return accuracy, losses, all_predictions, all_targets\n",
        "\n",
        "    return accuracy, losses"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c08ieH5BtJYL"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**HYPERPARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 1e-7\n",
        "EMBEDDING_DIM = 768 # 768 for base and 1024 for large\n",
        "LSTM_SIZE = 128\n",
        "DROPOUT = 0.2\n",
        "POOLED_EMB_DO = 0.3\n",
        "OUT_DIM_LIN = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(df_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_func)\n",
        "\n",
        "val_dataset = MyDataset(df_val)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_func)\n",
        "\n",
        "test_dataset = MyDataset(df_test)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajIRbYYqXUaE",
        "outputId": "d5fb8c98-64b4-4bdb-9648-df772fa53aac"
      },
      "outputs": [],
      "source": [
        "classifier = Classifier(lstm_size=LSTM_SIZE,\n",
        "                        emb_dim=EMBEDDING_DIM,\n",
        "                        out_dim=OUT_DIM_LIN,\n",
        "                        dropout= DROPOUT).to(device)\n",
        "\n",
        "tokenize = wrap_tokenizer(tokenizer)\n",
        "\n",
        "# If fine-tuning the model (FinBert)\n",
        "#params = list(model.parameters()) + list(classifier.parameters())\n",
        "\n",
        "# If using the base FinBert\n",
        "params = list(classifier.parameters())\n",
        "\n",
        "optimizer = AdamW(params, lr=5e-5)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=len(train_loader) * 1, \n",
        "    num_training_steps=len(train_loader) * EPOCHS)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TRAINING & VALIDATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giytCMUWTol1",
        "outputId": "9837e428-2ce9-4a07-f6d2-df44b4e83f41"
      },
      "outputs": [],
      "source": [
        "logs = {'train_acc':[],\n",
        "        'train_loss':[],\n",
        "        'val_acc':[],\n",
        "        'val_loss':[],\n",
        "        'lr': []}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_acc, train_loss = train(train_loader=train_loader,\n",
        "                                  tokenize=tokenize,\n",
        "                                  model=model,\n",
        "                                  classifier=classifier,\n",
        "                                  optimizer=optimizer,\n",
        "                                  scheduler=scheduler,\n",
        "                                  logs=logs,\n",
        "                                  dropout=POOLED_EMB_DO)\n",
        "\n",
        "                                  \n",
        "\n",
        "    val_acc, val_loss = val_step(val_loader=val_loader,\n",
        "                                 tokenize=tokenize,\n",
        "                                 model = model,\n",
        "                                 classifier = classifier,\n",
        "                                 device=device\n",
        "                                )\n",
        "\n",
        "    logs['train_acc'] += train_acc\n",
        "    logs['train_loss'] += train_loss\n",
        "    logs['val_acc'] += val_acc\n",
        "    logs['val_loss'] += val_loss\n",
        "\n",
        "    # Save checkpoint after each epoch\n",
        "    save_checkpoint(model = model, classifier = classifier, optimizer=optimizer, logs=logs, epoch=epoch)\n",
        "\n",
        "    print(f\"Epoch {epoch} --> train_loss:{mean(train_loss):.4f},\\\n",
        "                              train_acc:{mean(train_acc): .2f}%, \\\n",
        "                              val_loss:{mean(val_loss): .4f}, \\\n",
        "                              val_acc:{mean(val_acc): .2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "i7mx_2fgeYR4",
        "LcGYsPtZsKIi",
        "tNAimq0i3Sq5",
        "8fUZch4EU5_X",
        "KKswckthV_qX"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
