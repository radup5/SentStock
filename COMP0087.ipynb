{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8ob6w2AyrIVf"
      },
      "source": [
        "TO DO\n",
        "\n",
        "* refactor validation and test steps\n",
        "* probably need the finbert model in training mode too when training?\n",
        "* extend for multi-class classification - change loss function etc.\n",
        "* keeping LSTM for classification?\n",
        "* needs some hyperparameter tuning"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7cxggRBXn4Ne"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "H8Ri7UEW2wMk"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "g8kFjn0iGuCq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /opt/homebrew/lib/python3.11/site-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (1.26.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.1.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/homebrew/lib/python3.11/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in /opt/homebrew/lib/python3.11/site-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (0.21.4)\n",
            "Requirement already satisfied: packaging in /Users/andreeairina/Library/Python/3.11/lib/python/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/homebrew/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/andreeairina/Library/Python/3.11/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /opt/homebrew/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /Users/andreeairina/Library/Python/3.11/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: transformers in /opt/homebrew/lib/python3.11/site-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /opt/homebrew/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (1.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/andreeairina/Library/Python/3.11/lib/python/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests->transformers) (2023.11.17)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets\n",
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "urPT0BCWHr3q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from statistics import mean\n",
        "import pickle\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "r1WrqP_qHU9R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
        "from torch.optim import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sEXqEJ6X3wzQ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
        "finbert = AutoModel.from_pretrained(\"ProsusAI/finbert\").to(device)\n",
        "# bert = AutoModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "# distilbert = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lB2Jbs42Uc4J"
      },
      "source": [
        "SET THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7qTrC0eLUZ-o"
      },
      "outputs": [],
      "source": [
        "model = finbert"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "l0D4PPi_Gd_b"
      },
      "source": [
        "# Data loading and cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "9RxZlK3sdRPj",
        "outputId": "78876214-45f9-4b05-fa4c-f2fe1b5f9328"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>publish_time</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>returns</th>\n",
              "      <th>trend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-05-21</td>\n",
              "      <td>Costco, JPMorgan, Snowflake, Ford, Zoom, and M...</td>\n",
              "      <td>Earnings reports from Zoom Video, Lowe’s, Snow...</td>\n",
              "      <td>-0.162948</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-05-24</td>\n",
              "      <td>Agilent (A) Q2 Earnings Match Estimates, Reven...</td>\n",
              "      <td>Agilent Technologies A reported second-quarter...</td>\n",
              "      <td>-5.620333</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-06-13</td>\n",
              "      <td>Is it a Good Time to Invest in Agilent (A) Sha...</td>\n",
              "      <td>Baron Funds, an investment management firm, re...</td>\n",
              "      <td>1.223213</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>2023-11-20</td>\n",
              "      <td>Agilent Reports Fourth-Quarter Fiscal Year 202...</td>\n",
              "      <td>Revenue at the high end of guidance and EPS ex...</td>\n",
              "      <td>7.344231</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAL</td>\n",
              "      <td>2023-05-22</td>\n",
              "      <td>The Undoing of American and JetBlue’s Northeas...</td>\n",
              "      <td>American Airlines CEO Robert Isom (left) speak...</td>\n",
              "      <td>-3.171385</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3396</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-06-04</td>\n",
              "      <td>Zoetis' (NYSE:ZTS) five-year earnings growth t...</td>\n",
              "      <td>Stock pickers are generally looking for stocks...</td>\n",
              "      <td>0.337446</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3397</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-06-15</td>\n",
              "      <td>Zoetis Releases 2022 Sustainability Report to ...</td>\n",
              "      <td>$7.4 million in corporate giving invested in c...</td>\n",
              "      <td>3.972232</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3398</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-06-16</td>\n",
              "      <td>Zoetis’s Wetteny Joseph—one of the few Black C...</td>\n",
              "      <td>Good morning.“I was learning accounting about ...</td>\n",
              "      <td>-1.697479</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3399</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-06-21</td>\n",
              "      <td>What Makes Zoetis (ZTS) an Attractive Investme...</td>\n",
              "      <td>Aristotle Atlantic Partners, LLC, an investmen...</td>\n",
              "      <td>1.030129</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3400</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>2023-08-08</td>\n",
              "      <td>Zoetis (ZTS) Q2 2023 Earnings Call Transcript ...</td>\n",
              "      <td>Hosting the call today is Steve Frank, vice pr...</td>\n",
              "      <td>5.119265</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3401 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     symbol publish_time                                              title  \\\n",
              "0         A   2023-05-21  Costco, JPMorgan, Snowflake, Ford, Zoom, and M...   \n",
              "1         A   2023-05-24  Agilent (A) Q2 Earnings Match Estimates, Reven...   \n",
              "2         A   2023-06-13  Is it a Good Time to Invest in Agilent (A) Sha...   \n",
              "3         A   2023-11-20  Agilent Reports Fourth-Quarter Fiscal Year 202...   \n",
              "4       AAL   2023-05-22  The Undoing of American and JetBlue’s Northeas...   \n",
              "...     ...          ...                                                ...   \n",
              "3396    ZTS   2023-06-04  Zoetis' (NYSE:ZTS) five-year earnings growth t...   \n",
              "3397    ZTS   2023-06-15  Zoetis Releases 2022 Sustainability Report to ...   \n",
              "3398    ZTS   2023-06-16  Zoetis’s Wetteny Joseph—one of the few Black C...   \n",
              "3399    ZTS   2023-06-21  What Makes Zoetis (ZTS) an Attractive Investme...   \n",
              "3400    ZTS   2023-08-08  Zoetis (ZTS) Q2 2023 Earnings Call Transcript ...   \n",
              "\n",
              "                                                   body   returns     trend  \n",
              "0     Earnings reports from Zoom Video, Lowe’s, Snow... -0.162948    stable  \n",
              "1     Agilent Technologies A reported second-quarter... -5.620333  decrease  \n",
              "2     Baron Funds, an investment management firm, re...  1.223213    stable  \n",
              "3     Revenue at the high end of guidance and EPS ex...  7.344231  increase  \n",
              "4     American Airlines CEO Robert Isom (left) speak... -3.171385  decrease  \n",
              "...                                                 ...       ...       ...  \n",
              "3396  Stock pickers are generally looking for stocks...  0.337446    stable  \n",
              "3397  $7.4 million in corporate giving invested in c...  3.972232  increase  \n",
              "3398  Good morning.“I was learning accounting about ... -1.697479    stable  \n",
              "3399  Aristotle Atlantic Partners, LLC, an investmen...  1.030129    stable  \n",
              "3400  Hosting the call today is Steve Frank, vice pr...  5.119265  increase  \n",
              "\n",
              "[3401 rows x 6 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = pd.read_csv('data_nlp.csv')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "myzG63o5u3Sw",
        "outputId": "88dda2f2-a8ce-467c-c1cf-14a09faa03d1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>publish_time</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>returns</th>\n",
              "      <th>trend</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>symbol</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ABEO</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ADBE</th>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ATI</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ATXI</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AVXL</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BFAM</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BRKL</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CHTR</th>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DUOL</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MDXG</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NOV</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PDEX</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PNW</th>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RRC</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SCVL</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TMQ</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>WMT</th>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        publish_time  title  body  returns  trend\n",
              "symbol                                           \n",
              "ABEO               2      2     2        0      0\n",
              "ADBE              38     38    38        0      0\n",
              "ATI                2      2     2        0      0\n",
              "ATXI               2      2     2        0      0\n",
              "AVXL               2      2     2        0      0\n",
              "BFAM               2      2     2        0      0\n",
              "BRKL               2      2     2        0      0\n",
              "CHTR              20     20    20        0      0\n",
              "DUOL               1      1     1        0      0\n",
              "MDXG               2      2     2        0      0\n",
              "NOV                1      1     1        0      0\n",
              "PDEX               2      2     2        0      0\n",
              "PNW               10     10    10        0      0\n",
              "RRC                2      2     2        0      0\n",
              "SCVL               1      1     1        0      0\n",
              "TMQ                2      2     2        0      0\n",
              "WMT               42     42    42        0      0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nans = dataset[~dataset['trend'].isin(['stable', 'decrease', 'increase'])]\n",
        "nans.groupby(by='symbol').count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QEyfN4L2M26",
        "outputId": "41c52516-3b05-4595-df4b-48981d7ad015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "133"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(nans)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "SBg2eobOeQ-H"
      },
      "outputs": [],
      "source": [
        "df = dataset[['symbol', 'title', 'body', 'trend']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "G3sV_1UYeslx",
        "outputId": "ddd5a335-a57f-4f23-83f4-95bbe34909da"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>trend</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A</td>\n",
              "      <td>Costco, JPMorgan, Snowflake, Ford, Zoom, and M...</td>\n",
              "      <td>Earnings reports from Zoom Video, Lowe’s, Snow...</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A</td>\n",
              "      <td>Agilent (A) Q2 Earnings Match Estimates, Reven...</td>\n",
              "      <td>Agilent Technologies A reported second-quarter...</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A</td>\n",
              "      <td>Is it a Good Time to Invest in Agilent (A) Sha...</td>\n",
              "      <td>Baron Funds, an investment management firm, re...</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A</td>\n",
              "      <td>Agilent Reports Fourth-Quarter Fiscal Year 202...</td>\n",
              "      <td>Revenue at the high end of guidance and EPS ex...</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAL</td>\n",
              "      <td>The Undoing of American and JetBlue’s Northeas...</td>\n",
              "      <td>American Airlines CEO Robert Isom (left) speak...</td>\n",
              "      <td>decrease</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3396</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>Zoetis' (NYSE:ZTS) five-year earnings growth t...</td>\n",
              "      <td>Stock pickers are generally looking for stocks...</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3397</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>Zoetis Releases 2022 Sustainability Report to ...</td>\n",
              "      <td>$7.4 million in corporate giving invested in c...</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3398</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>Zoetis’s Wetteny Joseph—one of the few Black C...</td>\n",
              "      <td>Good morning.“I was learning accounting about ...</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3399</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>What Makes Zoetis (ZTS) an Attractive Investme...</td>\n",
              "      <td>Aristotle Atlantic Partners, LLC, an investmen...</td>\n",
              "      <td>stable</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3400</th>\n",
              "      <td>ZTS</td>\n",
              "      <td>Zoetis (ZTS) Q2 2023 Earnings Call Transcript ...</td>\n",
              "      <td>Hosting the call today is Steve Frank, vice pr...</td>\n",
              "      <td>increase</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3268 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     symbol                                              title  \\\n",
              "0         A  Costco, JPMorgan, Snowflake, Ford, Zoom, and M...   \n",
              "1         A  Agilent (A) Q2 Earnings Match Estimates, Reven...   \n",
              "2         A  Is it a Good Time to Invest in Agilent (A) Sha...   \n",
              "3         A  Agilent Reports Fourth-Quarter Fiscal Year 202...   \n",
              "4       AAL  The Undoing of American and JetBlue’s Northeas...   \n",
              "...     ...                                                ...   \n",
              "3396    ZTS  Zoetis' (NYSE:ZTS) five-year earnings growth t...   \n",
              "3397    ZTS  Zoetis Releases 2022 Sustainability Report to ...   \n",
              "3398    ZTS  Zoetis’s Wetteny Joseph—one of the few Black C...   \n",
              "3399    ZTS  What Makes Zoetis (ZTS) an Attractive Investme...   \n",
              "3400    ZTS  Zoetis (ZTS) Q2 2023 Earnings Call Transcript ...   \n",
              "\n",
              "                                                   body     trend  \n",
              "0     Earnings reports from Zoom Video, Lowe’s, Snow...    stable  \n",
              "1     Agilent Technologies A reported second-quarter...  decrease  \n",
              "2     Baron Funds, an investment management firm, re...    stable  \n",
              "3     Revenue at the high end of guidance and EPS ex...  increase  \n",
              "4     American Airlines CEO Robert Isom (left) speak...  decrease  \n",
              "...                                                 ...       ...  \n",
              "3396  Stock pickers are generally looking for stocks...    stable  \n",
              "3397  $7.4 million in corporate giving invested in c...  increase  \n",
              "3398  Good morning.“I was learning accounting about ...    stable  \n",
              "3399  Aristotle Atlantic Partners, LLC, an investmen...    stable  \n",
              "3400  Hosting the call today is Steve Frank, vice pr...  increase  \n",
              "\n",
              "[3268 rows x 4 columns]"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# df.dropna(inplace = True)\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "TegDd_AP6Q5c"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "def get_smaller_dataset(df):\n",
        "\n",
        "  df = df.dropna()\n",
        "\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
        "\n",
        "  X = df.drop('trend', axis=1)\n",
        "  y = df['trend']\n",
        "\n",
        "  for train_index, test_index in sss.split(X, y):\n",
        "      X_sample, y_sample = X.iloc[test_index], y.iloc[test_index]\n",
        "\n",
        "  sample_df = X_sample.assign(trend=y_sample.values)\n",
        "  sample_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "  return sample_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "93jQ0RKJ6kWF",
        "outputId": "5fe7e10f-9214-408d-d387-8dacadfd7843"
      },
      "outputs": [],
      "source": [
        "df = get_smaller_dataset(df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i7mx_2fgeYR4"
      },
      "source": [
        "# Old data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "zj9Yx7ZpGd_b"
      },
      "outputs": [],
      "source": [
        "# dataset = load_dataset(\"edarchimbaud/news-stocks\")\n",
        "# dataset.set_format(type='pandas')\n",
        "\n",
        "# df = dataset['train'][:]\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "iWb-HH7tGd_b"
      },
      "outputs": [],
      "source": [
        "# df = df.drop(['publisher', 'url', 'uuid'], axis=1)\n",
        "# df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "qJneDucMGd_b"
      },
      "outputs": [],
      "source": [
        "# # select stocks\n",
        "# ticker = 'AAPL'\n",
        "\n",
        "# df = df[df['symbol'] == ticker]\n",
        "\n",
        "# df = df.reset_index(drop=True)\n",
        "# df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LcGYsPtZsKIi"
      },
      "source": [
        "# Load stock prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "HGCbPZ4ed_oX"
      },
      "outputs": [],
      "source": [
        "# # start date is 7 days before the earliest day from the df\n",
        "# # to make sure at least one trading day before is included\n",
        "# # (there might be weekend days/holidays etc.)\n",
        "\n",
        "# start_date = df['publish_time'].min() - datetime.timedelta(7)\n",
        "# end_date = df['publish_time'].max() + datetime.timedelta(7)\n",
        "\n",
        "# prices = yf.download('AAPL', start_date, end_date)\n",
        "\n",
        "# prices.index = pd.to_datetime(prices.index, format='%Y-%m-%d', utc=True)\n",
        "# prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "3_FiB9RaSPBe"
      },
      "outputs": [],
      "source": [
        "# def trend(date):\n",
        "\n",
        "#   prev_date = prices[prices.index < date].index.max()\n",
        "#   prev_date_index = prices.index.get_loc(prev_date.strftime('%Y-%m-%d'))\n",
        "\n",
        "#   if date.strftime('%Y-%m-%d') == prev_date.strftime('%Y-%m-%d'):\n",
        "#     prev_date_index -= 1\n",
        "\n",
        "#   next_date = prices[prices.index > date].index.min()\n",
        "#   next_date_index = prices.index.get_loc(next_date.strftime('%Y-%m-%d'))\n",
        "#   ret = ((prices['Open'][next_date_index] - prices['Close'][prev_date_index]) / prices['Close'][prev_date_index]) * 100\n",
        "\n",
        "#   return_threshold = 1.0 # (1%)\n",
        "#   if ret >= return_threshold:\n",
        "#     return 'increase'\n",
        "#   elif ret <= -return_threshold:\n",
        "#     return 'decrease'\n",
        "#   else:\n",
        "#     return 'stable'\n",
        "\n",
        "\n",
        "# df['trend'] = df['publish_time'].apply(trend)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tNAimq0i3Sq5"
      },
      "source": [
        "# Tokenize data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "vrfxnago3UNv"
      },
      "outputs": [],
      "source": [
        "def wrap_tokenizer(tokenizer, padding=True, truncation=True, return_tensors='pt', max_length=None):\n",
        "    def tokenize(text):\n",
        "        text = list(text)\n",
        "        tokens = tokenizer(\n",
        "            text,\n",
        "            padding=padding,\n",
        "            return_attention_mask=False,\n",
        "            truncation=truncation,\n",
        "            max_length=max_length,\n",
        "            return_tensors=return_tensors\n",
        "            )['input_ids']\n",
        "        return tokens\n",
        "    return tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DUM2sou53cE7"
      },
      "outputs": [],
      "source": [
        "tokenize = wrap_tokenizer(tokenizer, padding=False, truncation = False, return_tensors=None)\n",
        "tokens = tokenize(df['body'])\n",
        "num_tokens = [len(x) for x in tokens]\n",
        "df['length'] = pd.Series(num_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "ZmcStF7gxmYh",
        "outputId": "7341b5c0-d9b3-43b4-d484-c06fd2f0c1c4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>trend</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMX</td>\n",
              "      <td>Used-car market in ‘uncharted territory’ as hi...</td>\n",
              "      <td>With spring car-buying season in full swing, t...</td>\n",
              "      <td>increase</td>\n",
              "      <td>752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>CI</td>\n",
              "      <td>Strong Forecast from UnitedHealth Sends Insure...</td>\n",
              "      <td>UnitedHealth Group boosted its profit outlook....</td>\n",
              "      <td>increase</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FEMY</td>\n",
              "      <td>Femasys Inc. Secures Additional Financing with...</td>\n",
              "      <td>Femasys Inc.- $6.85 million upfront investment...</td>\n",
              "      <td>increase</td>\n",
              "      <td>2871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MMM</td>\n",
              "      <td>U.S. News &amp; World Report incorporates 3M Ambul...</td>\n",
              "      <td>ST. PAUL, Minn., July 13, 2023 /PRNewswire/ --...</td>\n",
              "      <td>stable</td>\n",
              "      <td>1715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>TTI</td>\n",
              "      <td>TETRA Technologies Insider Buyers Net US$98k D...</td>\n",
              "      <td>Insiders who bought TETRA Technologies, Inc. (...</td>\n",
              "      <td>decrease</td>\n",
              "      <td>799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  symbol                                              title  \\\n",
              "0    KMX  Used-car market in ‘uncharted territory’ as hi...   \n",
              "1     CI  Strong Forecast from UnitedHealth Sends Insure...   \n",
              "2   FEMY  Femasys Inc. Secures Additional Financing with...   \n",
              "3    MMM  U.S. News & World Report incorporates 3M Ambul...   \n",
              "4    TTI  TETRA Technologies Insider Buyers Net US$98k D...   \n",
              "\n",
              "                                                body     trend  length  \n",
              "0  With spring car-buying season in full swing, t...  increase     752  \n",
              "1  UnitedHealth Group boosted its profit outlook....  increase      92  \n",
              "2  Femasys Inc.- $6.85 million upfront investment...  increase    2871  \n",
              "3  ST. PAUL, Minn., July 13, 2023 /PRNewswire/ --...    stable    1715  \n",
              "4  Insiders who bought TETRA Technologies, Inc. (...  decrease     799  "
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = df.iloc[:50]\n",
        "# df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXQ8naoTecrt",
        "outputId": "8da277dd-5a64-465b-9dec-faf44a0f1035"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16358"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_length = df['length'].unique().max()\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GObBDlbl3gD8",
        "outputId": "fbcce4b7-744b-45fa-a9be-31f650949d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of text that have more than 1*max_tokens is 662\n",
            "Number of text that have more than 2*max_tokens is 429\n",
            "Number of text that have more than 3*max_tokens is 275\n",
            "Number of text that have more than 4*max_tokens is 197\n",
            "Number of text that have more than 5*max_tokens is 148\n",
            "Number of text that have more than 6*max_tokens is 100\n",
            "Number of text that have more than 7*max_tokens is 73\n",
            "Number of text that have more than 8*max_tokens is 53\n",
            "Number of text that have more than 9*max_tokens is 38\n",
            "Number of text that have more than 10*max_tokens is 34\n",
            "Number of text that have more than 11*max_tokens is 24\n",
            "Number of text that have more than 12*max_tokens is 19\n",
            "Number of text that have more than 13*max_tokens is 16\n",
            "Number of text that have more than 14*max_tokens is 10\n",
            "Number of text that have more than 15*max_tokens is 10\n",
            "Number of text that have more than 16*max_tokens is 9\n",
            "Number of text that have more than 17*max_tokens is 7\n",
            "Number of text that have more than 18*max_tokens is 5\n",
            "Number of text that have more than 19*max_tokens is 4\n",
            "Number of text that have more than 20*max_tokens is 4\n",
            "Number of text that have more than 21*max_tokens is 2\n",
            "Number of text that have more than 22*max_tokens is 2\n",
            "Number of text that have more than 23*max_tokens is 2\n",
            "Number of text that have more than 24*max_tokens is 1\n",
            "Number of text that have more than 25*max_tokens is 1\n",
            "Number of text that have more than 26*max_tokens is 1\n",
            "Number of text that have more than 27*max_tokens is 1\n",
            "Number of text that have more than 28*max_tokens is 1\n",
            "Number of text that have more than 29*max_tokens is 1\n",
            "Number of text that have more than 30*max_tokens is 1\n"
          ]
        }
      ],
      "source": [
        "max_length = df['length'].unique().max()\n",
        "max_length\n",
        "max_tokens = 512\n",
        "for i in range(1,(max_length//max_tokens)):\n",
        "    num = sum(df['length']>i*max_tokens)\n",
        "    print(f\"Number of text that have more than {i}*max_tokens is {num}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "WIBus79rsMA8"
      },
      "outputs": [],
      "source": [
        "def get_text_split(text, length=200, overlap=50, max_chunks=4):\n",
        "    chunks = []\n",
        "\n",
        "    words = text.split()\n",
        "    n_words = len(words)\n",
        "\n",
        "    n = max(1, min(max_chunks, (n_words - length) // (length - overlap) + 1))\n",
        "\n",
        "    for i in range(n):\n",
        "        start_idx = i * (length - overlap)\n",
        "        end_idx = min(start_idx + length, n_words)\n",
        "\n",
        "        chunk_words = words[start_idx:end_idx]\n",
        "\n",
        "        chunk_text = \" \".join(chunk_words)\n",
        "\n",
        "        # If it's the last chunk and its length is less than 75% of the desired length, skip\n",
        "        if i == n - 1 and len(chunk_words) < 0.75 * length and n > 1:\n",
        "            continue\n",
        "\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lxsiO2hW3r0w"
      },
      "source": [
        "**ENCODE LABELS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "wEGeEU453rR9"
      },
      "outputs": [],
      "source": [
        "labels = [\"increase\", \"stable\", \"decrease\"]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "df['trend'] = label_encoder.fit_transform(df['trend'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8fUZch4EU5_X"
      },
      "source": [
        "# Train-val-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "ln0P6JyJVDRV"
      },
      "outputs": [],
      "source": [
        "def split_df(df):\n",
        "    n_rows = len(df)\n",
        "\n",
        "    df_train = df.iloc[:int(0.8*n_rows),:]\n",
        "    df_val = df.iloc[int(0.8*n_rows):int(0.9*n_rows),:]\n",
        "    df_test = df.iloc[int(0.9*n_rows):,:]\n",
        "\n",
        "    return df_train, df_val, df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17TPVuLvVBZz",
        "outputId": "b4f2c83c-592e-440c-f492-8aa35e3f627f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples: 652\n",
            "Number of validation examples: 82\n",
            "Number of test examples: 83\n"
          ]
        }
      ],
      "source": [
        "n_rows = len(df)\n",
        "dfs_train, dfs_val, dfs_test = [],[],[]\n",
        "gb = df.groupby('trend')\n",
        "for x in gb.groups:\n",
        "    group = gb.get_group(x)\n",
        "    df_train, df_val, df_test = split_df(group)\n",
        "    dfs_train.append(df_train)\n",
        "    dfs_val.append(df_val)\n",
        "    dfs_test.append(df_test)\n",
        "\n",
        "df_train = pd.concat(dfs_train, ignore_index=True)\n",
        "\n",
        "df_val = pd.concat(dfs_val, ignore_index=True)\n",
        "\n",
        "df_test = pd.concat(dfs_test, ignore_index=True)\n",
        "\n",
        "print(f'Number of training examples: {len(df_train)}')\n",
        "print(f'Number of validation examples: {len(df_val)}')\n",
        "print(f'Number of test examples: {len(df_test)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "WanNgS0RU5Ns"
      },
      "outputs": [],
      "source": [
        "df_train.body = df_train.body.apply(lambda x: get_text_split(x))\n",
        "df_val.body = df_val.body.apply(lambda x: get_text_split(x))\n",
        "df_test.body = df_test.body.apply(lambda x: get_text_split(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ZJwTu6xVVHrn"
      },
      "outputs": [],
      "source": [
        "df_train['n_chunks'] = df_train.body.apply(lambda x: len(x))\n",
        "df_val['n_chunks'] = df_val.body.apply(lambda x: len(x))\n",
        "df_test['n_chunks'] = df_test.body.apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "8_1IexNxymQO",
        "outputId": "a3b1fd9e-520d-47fe-8117-71901a243b39"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>trend</th>\n",
              "      <th>length</th>\n",
              "      <th>n_chunks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TTI</td>\n",
              "      <td>TETRA Technologies Insider Buyers Net US$98k D...</td>\n",
              "      <td>[Insiders who bought TETRA Technologies, Inc. ...</td>\n",
              "      <td>0</td>\n",
              "      <td>799</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>RSG</td>\n",
              "      <td>Republic Services, Inc. (NYSE:RSG) Delivered A...</td>\n",
              "      <td>[Many investors are still learning about the v...</td>\n",
              "      <td>0</td>\n",
              "      <td>1679</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GSIT</td>\n",
              "      <td>GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ear...</td>\n",
              "      <td>[GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ea...</td>\n",
              "      <td>0</td>\n",
              "      <td>2126</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ARE</td>\n",
              "      <td>Alexandria Real Estate Equities, Inc.'s Discip...</td>\n",
              "      <td>[PASADENA, Calif., May 24, 2023 /PRNewswire/ -...</td>\n",
              "      <td>0</td>\n",
              "      <td>1437</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MO</td>\n",
              "      <td>Altria (MO) Gains on Pricing Power Amid Low Ci...</td>\n",
              "      <td>[Altria Group, Inc. MO looks well-positioned d...</td>\n",
              "      <td>0</td>\n",
              "      <td>1328</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  symbol                                              title  \\\n",
              "0    TTI  TETRA Technologies Insider Buyers Net US$98k D...   \n",
              "1    RSG  Republic Services, Inc. (NYSE:RSG) Delivered A...   \n",
              "2   GSIT  GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ear...   \n",
              "3    ARE  Alexandria Real Estate Equities, Inc.'s Discip...   \n",
              "4     MO  Altria (MO) Gains on Pricing Power Amid Low Ci...   \n",
              "\n",
              "                                                body  trend  length  n_chunks  \n",
              "0  [Insiders who bought TETRA Technologies, Inc. ...      0     799         3  \n",
              "1  [Many investors are still learning about the v...      0    1679         4  \n",
              "2  [GSI Technology, Inc. (NASDAQ:GSIT) Q2 2024 Ea...      0    2126         4  \n",
              "3  [PASADENA, Calif., May 24, 2023 /PRNewswire/ -...      0    1437         4  \n",
              "4  [Altria Group, Inc. MO looks well-positioned d...      0    1328         4  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KKswckthV_qX"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "s1YnbShGacx3"
      },
      "source": [
        "HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "wejQ8_Bzad-8"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 10\n",
        "EPOCHS = 5\n",
        "LR = 1e-7\n",
        "EMBEDDING_DIM = 768 # 768 for base and 1024 for large\n",
        "LSTM_SIZE = 128\n",
        "LSTM_DO = 0.2\n",
        "POOLED_EMB_DO = 0.3\n",
        "OUT_DIM_LIN = 3\n",
        "WEIGHT_DECAY = 1e-3\n",
        "#=============================\n",
        "# freeze xlmr hyperparams\n",
        "EPOCHS_FREEZE = 2\n",
        "NUM_WARMUP_EPOCHS_FREEZE = 1\n",
        "LR_FREEZE = 6e-5\n",
        "#=============================\n",
        "LR_LIN = 3e-6\n",
        "EPOCHS_LIN = 50\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "VzmXJzFxWAez"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self,df):\n",
        "        self.n_chunks = df['n_chunks'].to_list()\n",
        "        self.X = df['body'].to_list()\n",
        "        self.Y = df['trend']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.X[index], self.Y.iloc[index], self.n_chunks[index]\n",
        "\n",
        "def collate_func(batch):\n",
        "    X = [x[0] for x in batch]\n",
        "    Y = torch.Tensor([x[1] for x in batch])\n",
        "    c = [x[2] for x in batch]\n",
        "    return [X,Y,c]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "IU-F8-RijOJL"
      },
      "outputs": [],
      "source": [
        "class Classifier(nn.Module):\n",
        "    def __init__(self, lstm_size, emb_dim, out_dim_lin, lstm_do):\n",
        "        super().__init__()\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=emb_dim, hidden_size=lstm_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(lstm_do)\n",
        "        self.linear = nn.Linear(in_features=lstm_size, out_features=out_dim_lin)\n",
        "\n",
        "    def forward(self, x, n_chunks):\n",
        "        x = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "        x = pack_padded_sequence(input=x, lengths=n_chunks, batch_first=True, enforce_sorted=False)\n",
        "        x, _ = self.lstm(x)\n",
        "        x, _ = pad_packed_sequence(x, batch_first=True)\n",
        "        x = x[:,-1,:]\n",
        "        x = self.dropout(x)\n",
        "        logit = self.linear(x)\n",
        "        return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "WAY544k5eIDU"
      },
      "outputs": [],
      "source": [
        "# class Classifier(nn.Module):\n",
        "#     def __init__(self, bert_model, out_dim_lin, dropout):\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.bert = bert_model\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.linear = nn.Linear(in_features=self.bert.config.hidden_size, out_features=out_dim_lin)\n",
        "\n",
        "#     def forward(self, x, n_chunks):\n",
        "\n",
        "#         x = pad_sequence(x, batch_first=True, padding_value=0)\n",
        "\n",
        "#         with torch.no_grad():\n",
        "#             bert_outputs = self.bert(x)\n",
        "\n",
        "#         bert_last_hidden_state = bert_outputs.last_hidden_state\n",
        "\n",
        "#         x = self.dropout(bert_last_hidden_state)\n",
        "\n",
        "#         logit = self.linear(x[:, 0, :])\n",
        "\n",
        "#         return logit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "cporksIYXdDc"
      },
      "outputs": [],
      "source": [
        "# def save_to_disk(txt_path, values):\n",
        "#     if os.path.isfile(txt_path):\n",
        "#         os.remove(txt_path)\n",
        "#     with open(txt_path, \"wb\") as fp:\n",
        "#         pickle.dump(values, fp)\n",
        "#     return\n",
        "\n",
        "\n",
        "# def load_from_disk(txt_path):\n",
        "#     with open(txt_path, \"rb\") as f:\n",
        "#         values =  pickle.load(f)\n",
        "#     return values\n",
        "\n",
        "\n",
        "# def save_checkpoint(xlmr, classifier, optimizer, logs, checkpoint_dir, epoch):\n",
        "#     print('')\n",
        "#     print('Saving checkpoint...')\n",
        "#     state_dict = {\n",
        "#         'classifier':classifier.state_dict(),\n",
        "#         'optimizer': optimizer.state_dict(),\n",
        "#     }\n",
        "#     torch.save(state_dict, os.path.join(checkpoint_dir, 'checkpoint_{}.pt'.format(epoch)))\n",
        "#     save_to_disk(os.path.join(checkpoint_dir, 'logs.txt'),logs)\n",
        "#     print(f'Checkpoint saved!')\n",
        "\n",
        "# checkpoint_dir = '/content/google_drive/MyDrive/checkpoints'\n",
        "\n",
        "# def load_checkpoint(checkpoint_dir, epoch, xlmr, classifier, device, optimizer=None):\n",
        "#     pretrained_dict = torch.load(os.path.join(checkpoint_dir,'checkpoint_{}.pt'.format(epoch)),map_location=torch.device(device))\n",
        "#     classifier.load_state_dict(pretrained_dict['classifier'])\n",
        "#     if optimizer is not None:\n",
        "#         optimizer.load_state_dict(pretrained_dict['optimizer'])\n",
        "#         return classifier, optimizer\n",
        "#     return classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "oRHHICtbWM-a"
      },
      "outputs": [],
      "source": [
        "# def get_lr(optimizer):\n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         return param_group['lr']\n",
        "\n",
        "# def set_lr(optimizer, lr):\n",
        "#     for param_group in optimizer.param_groups:\n",
        "#         param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "6tjPUZ-naSeX"
      },
      "outputs": [],
      "source": [
        "# def train(train_loader, tokenize, device, model, optimizer, classifier, dropout=0.0):\n",
        "\n",
        "#     classifier.train()\n",
        "#     model.train()\n",
        "\n",
        "#     accuracy = []\n",
        "#     losses = []\n",
        "\n",
        "#     for text, target, n_chunks in train_loader:\n",
        "#         target = target.to(device)\n",
        "\n",
        "#         flat_text = [item for sublist in text for item in sublist]\n",
        "#         tokens = tokenize(flat_text)\n",
        "#         tokens = tokens.to(device)\n",
        "\n",
        "#         # Based on the results - I think we should finetune the model\n",
        "#         # with torch.no_grad():\n",
        "#         #   outputs = model(tokens)\n",
        "#         outputs = model(tokens)\n",
        "\n",
        "#         embeddings = outputs.last_hidden_state\n",
        "\n",
        "#         pooled_emb = torch.mean(embeddings, axis=1)\n",
        "#         pooled_emb = nn.Dropout(dropout)(pooled_emb)\n",
        "#         x = [s for s in torch.split(pooled_emb, n_chunks, dim=0)]\n",
        "\n",
        "#         logit = classifier(x, n_chunks)\n",
        "#         prob = torch.sigmoid(logit)\n",
        "\n",
        "#         target = torch.reshape(target, shape=(-1, 1))\n",
        "#         loss = nn.BCELoss()(input=prob, target=target.float())\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         prediction = (prob >= 0.5).float()\n",
        "#         correct_predictions = torch.sum(prediction == target).item()\n",
        "#         acc = correct_predictions / len(target)\n",
        "#         accuracy.append(acc)\n",
        "#         losses.append(loss.item())\n",
        "\n",
        "#     return accuracy, losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yrvvsE6hW0c1"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, tokenize, device, model, optimizer, classifier, dropout=0.0):\n",
        "    classifier.train()\n",
        "    model.train()\n",
        "\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "\n",
        "    for text, target, n_chunks in train_loader:\n",
        "        print('!!!')\n",
        "        target = target.long().to(device)\n",
        "\n",
        "        flat_text = [item for sublist in text for item in sublist]\n",
        "        tokens = tokenize(flat_text)\n",
        "        tokens = tokens.to(device)\n",
        "\n",
        "        # with torch.no_grad():\n",
        "        #   outputs = model(tokens)\n",
        "\n",
        "        outputs = model(tokens)\n",
        "        embeddings = outputs.last_hidden_state\n",
        "\n",
        "        pooled_emb = torch.mean(embeddings, axis=1)\n",
        "        pooled_emb = nn.Dropout(dropout)(pooled_emb)\n",
        "        x = [s for s in torch.split(pooled_emb, n_chunks, dim=0)]\n",
        "\n",
        "        logits = classifier(x, n_chunks)\n",
        "\n",
        "        loss = nn.CrossEntropyLoss()(input=logits, target=target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        _, predictions = torch.max(logits, 1)\n",
        "        correct_predictions = torch.sum(predictions == target).item()\n",
        "        acc = correct_predictions / target.size(0)\n",
        "        accuracy.append(acc)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    return accuracy, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "cTqXzoqdWEn-"
      },
      "outputs": [],
      "source": [
        "def val_step(val_loader, tokenize, device, model, classifier, is_test=False):\n",
        "\n",
        "    classifier.eval()\n",
        "    model.eval()\n",
        "\n",
        "    accuracy = []\n",
        "    losses = []\n",
        "\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text, target, n_chunks in val_loader:\n",
        "            target = target.to(device)\n",
        "\n",
        "            flat_text = [item for sublist in text for item in sublist]\n",
        "            tokens = tokenize(flat_text)\n",
        "            tokens = tokens.to(device)\n",
        "\n",
        "            outputs = model(tokens)\n",
        "            embeddings = outputs.last_hidden_state\n",
        "\n",
        "            pooled_emb = torch.mean(embeddings, axis=1)\n",
        "            x = [s for s in torch.split(pooled_emb, n_chunks, dim=0)]\n",
        "\n",
        "            logits = classifier(x, n_chunks)\n",
        "\n",
        "            loss = nn.CrossEntropyLoss()(input=logits, target=target)\n",
        "\n",
        "            _, predictions = torch.max(logits, 1)\n",
        "            correct_predictions = torch.sum(predictions == target).item()\n",
        "            acc = correct_predictions / target.size(0)\n",
        "            accuracy.append(acc)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            if is_test:\n",
        "                all_predictions.append(predictions.cpu())\n",
        "                all_targets.append(target.cpu())\n",
        "\n",
        "    if is_test:\n",
        "        all_predictions = torch.cat(all_predictions).numpy()\n",
        "        all_targets = torch.cat(all_targets).numpy()\n",
        "        return accuracy, losses, all_predictions, all_targets\n",
        "\n",
        "    return accuracy, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "1AZQ9xLHWgjU"
      },
      "outputs": [],
      "source": [
        "train_dataset = MyDataset(df_train)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, collate_fn=collate_func)\n",
        "\n",
        "val_dataset = MyDataset(df_val)\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, collate_fn=collate_func)\n",
        "\n",
        "test_dataset = MyDataset(df_test)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_func)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c08ieH5BtJYL"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajIRbYYqXUaE",
        "outputId": "d5fb8c98-64b4-4bdb-9648-df772fa53aac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected device is cpu\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# checkpoint_dir = '/content/google_drive/MyDrive/checkpoints'\n",
        "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Selected device is {}'.format(device))\n",
        "\n",
        "classifier = Classifier(lstm_size=LSTM_SIZE,\n",
        "                        emb_dim=EMBEDDING_DIM,\n",
        "                        out_dim_lin=OUT_DIM_LIN,\n",
        "                        lstm_do = LSTM_DO).to(device)\n",
        "\n",
        "\n",
        "params = list(model.parameters()) + list(classifier.parameters())\n",
        "#params = list(classifier.parameters())\n",
        "# optimizer = SGD(params, lr=LR_FREEZE, weight_decay=WEIGHT_DECAY)\n",
        "optimizer = AdamW(params, lr=5e-5)\n",
        "\n",
        "tokenize = wrap_tokenizer(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giytCMUWTol1",
        "outputId": "9837e428-2ce9-4a07-f6d2-df44b4e83f41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "Epoch 0 --> loss:0.7820,                               acc: 0.6379% \n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n",
            "!!!\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[61], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m logs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mtrain_acc\u001b[39m\u001b[39m'\u001b[39m:[],\u001b[39m'\u001b[39m\u001b[39mtrain_loss\u001b[39m\u001b[39m'\u001b[39m:[],\n\u001b[1;32m      2\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mval_acc\u001b[39m\u001b[39m'\u001b[39m:[],\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m:[]}\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(EPOCHS):\n\u001b[0;32m----> 6\u001b[0m     train_acc, train_loss \u001b[39m=\u001b[39m train(train_loader \u001b[39m=\u001b[39;49m train_loader,\n\u001b[1;32m      7\u001b[0m                                   tokenize \u001b[39m=\u001b[39;49m tokenize,\n\u001b[1;32m      8\u001b[0m                                   device \u001b[39m=\u001b[39;49m device,\n\u001b[1;32m      9\u001b[0m                                   optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m     10\u001b[0m                                   model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m     11\u001b[0m                                   classifier \u001b[39m=\u001b[39;49m classifier,\n\u001b[1;32m     12\u001b[0m                                   dropout\u001b[39m=\u001b[39;49mPOOLED_EMB_DO)\n\u001b[1;32m     14\u001b[0m     \u001b[39m# val_acc, val_loss, val_cm = val_step(\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m#     val_loader=val_loader,\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m#     tokenize=tokenize,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[39m#     classifier=classifier\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[39m# )\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     logs[\u001b[39m'\u001b[39m\u001b[39mtrain_acc\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train_acc\n",
            "Cell \u001b[0;32mIn[57], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, tokenize, device, model, optimizer, classifier, dropout)\u001b[0m\n\u001b[1;32m     14\u001b[0m tokens \u001b[39m=\u001b[39m tokens\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m#   outputs = model(tokens)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[39m=\u001b[39m model(tokens)\n\u001b[1;32m     20\u001b[0m embeddings \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     22\u001b[0m pooled_emb \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(embeddings, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:309\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey(hidden_states))\n\u001b[0;32m--> 309\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalue(hidden_states))\n\u001b[1;32m    311\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n\u001b[1;32m    313\u001b[0m use_cache \u001b[39m=\u001b[39m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "logs = {'train_acc':[],'train_loss':[],\n",
        "        'val_acc':[],'val_loss':[]}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_acc, train_loss = train(train_loader = train_loader,\n",
        "                                  tokenize = tokenize,\n",
        "                                  device = device,\n",
        "                                  optimizer = optimizer,\n",
        "                                  model = model,\n",
        "                                  classifier = classifier,\n",
        "                                  dropout=POOLED_EMB_DO)\n",
        "\n",
        "    # val_acc, val_loss, val_cm = val_step(\n",
        "    #     val_loader=val_loader,\n",
        "    #     tokenize=tokenize,\n",
        "    #     device=device,\n",
        "    #     xlmr=xlmr,\n",
        "    #     classifier=classifier\n",
        "    # )\n",
        "\n",
        "    logs['train_acc'] += train_acc\n",
        "    logs['train_loss'] += train_loss\n",
        "    # logs['val_acc'] += val_acc\n",
        "    # logs['val_loss'] += val_loss\n",
        "\n",
        "    # if epoch % 2 == 0 and epoch != 0 :\n",
        "    #     save_checkpoint(\n",
        "    #         xlmr=xlmr,\n",
        "    #         classifier=classifier,\n",
        "    #         optimizer=optimizer,\n",
        "    #         logs=logs,\n",
        "    #         checkpoint_dir=checkpoint_dir,\n",
        "    #         epoch=epoch\n",
        "    #         )\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} --> loss:{mean(train_loss):.4f},\\\n",
        "                               acc:{mean(train_acc): .4f}% \")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGjBsDpVn7wt"
      },
      "source": [
        "FinBert (training)\n",
        "\n",
        "* Epoch 0 --> loss:0.6814,                               acc: 0.6089%\n",
        "* Epoch 1 --> loss:0.6841,                               acc: 0.5881%\n",
        "* Epoch 2 --> loss:0.6828,                               acc: 0.6600%\n",
        "* Epoch 3 --> loss:0.6844,                               acc: 0.5881%\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lTuawvctbnMZ"
      },
      "source": [
        "Distil Bert (training)\n",
        "\n",
        "* Epoch 0 --> loss:0.6897,                               acc: 0.5028%\n",
        "* Epoch 1 --> loss:0.6923,                               acc: 0.4934%\n",
        "* Epoch 2 --> loss:0.6905,                               acc: 0.5038%\n",
        "* Epoch 3 --> loss:0.6934,                               acc: 0.4839%\n",
        "* Epoch 4 --> loss:0.6882,                               acc: 0.5350%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "i7mx_2fgeYR4",
        "LcGYsPtZsKIi",
        "tNAimq0i3Sq5",
        "8fUZch4EU5_X",
        "KKswckthV_qX"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
